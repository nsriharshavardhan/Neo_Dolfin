# -*- coding: utf-8 -*-
"""DolFin_Feedback_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rHKJVxGRIZ3-5A3Pwc26fYr8dhtx4Ei9
"""

!pip install tensorflow pandas nltk wordcloud datasets

# Processing Utilities
import datetime
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Natural Language Processing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Deep learning
import tensorflow as tf
from tensorflow import keras
from keras import Model, layers
from keras.models import Sequential
from transformers import BertTokenizer, TFBertForSequenceClassification

# Plotting utillities
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from wordcloud import WordCloud

# # Mount Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# Data location, batch and image size
dataset_dir = ''
file_name = 'reviews.csv'

# Load the dataset into a dataframe
df = pd.read_csv(dataset_dir + file_name)

# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)

# Store the column names
labels = list(df.columns)

# Determine the shape of the data
num_samples = df.shape[0]
num_features = df.shape[1]

msg = f'The file {file_name} contains {num_features} features and {num_samples} samples \n'
msg += f'The column names are: {labels}'
print(msg)

# Ensure the content is a string datatype
df['content'] = df['content'].astype(str)

# Display the datatypes for each of the features
df.dtypes

# Modify the map_sentiment function to classify sentiments into 'good', 'neutral', and 'bad'
def map_sentiment(score):
    if score < 3.0:
        sentiment = 'Bad'
    elif score > 3.5:
        sentiment = 'Good'
    else:
        sentiment = 'Neutral'
    return sentiment

# Apply the modified function to create multi-class sentiment labels
df['sentiment'] = df['score'].apply(map_sentiment)

# Create a stop words variable
stop_words = set(stopwords.words('english'))

# Preprocess the text in the content field
def preprocess_content(text):
    # Covert the text to lower case
    tokens = word_tokenize(text.lower())
    # Remove stop words from the text and ensure each token is seproate by a space
    result = ' '.join([word for word in tokens if word.isalpha() and word not in stop_words])
    return result

# Apply the preprocessing transformation
df['processed_text'] = df['content'].apply(preprocess_content)

# Tokenisation of the processed_text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['processed_text'])
sequences = tokenizer.texts_to_sequences(df['processed_text'])
padded_sequences = pad_sequences(sequences, maxlen=200)

# Mapping sentiments to integers
sentiment_label = df.sentiment.factorize()
label_index = dict((i, label) for label, i in enumerate(df.sentiment.unique()))

def transform_array(arr):
    # Create an empty 2D array with zeros
    reshaped_array = np.zeros((len(arr), len(np.unique(arr))), dtype=int)

    # Set the appropriate positions to 1
    for i, val in enumerate(arr):
        reshaped_array[i, val] = 1

    return reshaped_array

input_array = sentiment_label[0]
result_array = transform_array(input_array)
print(result_array)

# Specify the split into train, test, split
train_split = 0.7
test_split = 0.15
val_split = 0.15

# Determine the number of samples
train_samples = int(train_split * num_samples)
test_samples = int(test_split * num_samples)
val_samples = num_samples - test_samples - train_samples

x_train, x_temp, y_train, y_temp = train_test_split(padded_sequences, result_array, test_size=0.3, random_state=42)
x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)

# Create a tensorboard callback
log_dir = os.path.join("logs", "fit", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Create a early stopping callback to prevent overfitting
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)

tf_callbacks = [tensorboard_callback, early_stopping]

# Develop the model
dropouts = [0.1, 0.2, 0.3]
dense_units = [12, 16, 20]
accuracies = []
models = []
for dropout in dropouts:
    for dense_unit in dense_units:
        model = tf.keras.Sequential([
            layers.Embedding(5000, 64, input_length=200),
            layers.Bidirectional(layers.LSTM(64)),
            layers.Dense(64, activation='relu'),
            layers.Dropout(dropout),
            layers.Dense(dense_unit, activation='relu', kernel_initializer='he_uniform'),
            layers.Dense(3, activation = 'softmax')
        ])

        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

        model.summary()

        # Specify the maximum number of epochs
        num_epochs=1

        # Fit the model to the training data
        history = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_val, y_val), callbacks=tf_callbacks)
        accuracy = history.history['val_accuracy']
        accuracies.extend(accuracy)
        # Create a dictionary with model name, accuracy, dropout, and dense unit
        model_dict = {
            "model_name": model.name,
            "accuracy": accuracy,
            "dropout": dropout,
            "dense_unit": dense_unit
        }

        # Append the dictionary to the `models` list
        models.append(model_dict)

# TensorFlow callback for logging hyperparameters
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Save the best performing model based on validation score
best_model_accuracy = max(accuracies)
best_model_path = 'best_model.h5'
for model_dict in models:
    current_accuracy = max(model_dict["accuracy"])
    if current_accuracy == best_model_accuracy:
        best_model = Sequential(name=model_dict["model_name"])
        best_dropout = model_dict['dropout']
        best_dense_unit = model_dict['dense_unit']
        print("Best validation accuracy:", best_model_accuracy)

best_model = tf.keras.Sequential([
            layers.Embedding(5000, 64, input_length=200),
            layers.Bidirectional(layers.LSTM(64)),
            layers.Dense(64, activation='relu'),
            layers.Dropout(best_dropout),
            layers.Dense(best_dense_unit, activation='relu', kernel_initializer='he_uniform'),
            layers.Dense(3, activation = 'softmax')
        ])

best_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Specify the maximum number of epochs
num_epochs=100

# Fit the model to the training data
history = best_model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_val, y_val), callbacks=tf_callbacks)

# Save a copy of the model
best_model.save_weights('best_model.h5')

# Predict the values in the test dataset
y_pred = best_model.predict(x_test)

# Calculate the model accuracy
loss, accuracy = best_model.evaluate(x_test, y_test)
print(f'Test Accuracy is {accuracy:.2f}')

# Find the maximum value in each row
max_values = y_pred.max(axis=1, keepdims=True)

# Create the binary indicator matrix
binary_matrix = (y_pred == max_values).astype(int)

print(binary_matrix)

def transform_array(arr):
    reshaped_array = []
    for i, val in enumerate(arr):
        reshaped_array.append(val.tolist().index(1))

    return reshaped_array

input_array = binary_matrix
y_pred_labels = transform_array(input_array)
y_test_labels = transform_array(y_test)

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

x_train_input_ids = x_train
x_val_input_ids = x_val

# Load the pre-trained BERT model for sequence classification
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Compile the model
bert_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
num_epochs=5
history_bert = bert_model.fit(x_train_input_ids, y_train, epochs=num_epochs, validation_data=(x_val_input_ids, y_val))

# Evaluate the model on validation data
val_loss, val_accuracy = bert_model.evaluate(x_val_input_ids, y_val)
print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}")

"""**Documentation**

- **Library Installation**: The code begins by installing necessary Python libraries such as TensorFlow, Pandas, NLTK, and WordCloud using the `pip install` command.

- **Processing Utilities**: It imports various modules for handling data, such as `datetime` for working with dates and times, `os` for interacting with the operating system, and `numpy` and `pandas` for numerical and data frame operations respectively.

- **Natural Language Processing (NLP)**: The notebook uses NLTK for text processing, such as tokenizing words and removing stopwords. TensorFlow's Keras API is utilized for text tokenization and sequence padding.

- **Deep Learning Model**: The code defines a deep learning model using TensorFlow's Keras API. It employs an embedding layer, a bidirectional LSTM layer, and several dense layers with dropout for regularization.

- **Data Preparation**: The dataset is loaded into a DataFrame, shuffled, and the column names are stored. The 'content' column is ensured to be of string datatype, and a new 'sentiment' column is created by applying a custom function to classify sentiments.

- **Text Preprocessing**: A function is defined to preprocess the text data by converting it to lowercase, tokenizing, and removing stopwords.

- **Model Training**: The notebook sets up a train-test-validation split, creates callbacks for TensorBoard and early stopping, and iterates over different configurations of dropout and dense units to train multiple models.

- **Model Evaluation**: After training, the models are evaluated on the validation set, and the best model is identified based on accuracy. The best model is then saved, and its performance is tested on the test set.

- **BERT Integration**: The notebook also includes code for initializing a BERT tokenizer and loading a pre-trained BERT model for sequence classification, although this part seems to be incomplete as there's no further code to train or evaluate the BERT model.

**Interpreting the Results**

The code outputs the validation loss, validation accuracy, and sentiment distribution. The word clouds and pie chart provide a visual representation of the sentiments.

**Conclusion**

The DolFin feedback sentiment analysis project aims to analyze customer feedback and determine the sentiment towards a product or service. The code implements a deep learning model using TensorFlow and Keras, and fine-tunes a pre-trained BERT model for sequence classification. The results are visualized using word clouds and a pie chart, providing insights into the sentiment distribution.
"""