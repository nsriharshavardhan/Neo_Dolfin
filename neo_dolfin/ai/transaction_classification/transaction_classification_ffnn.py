# -*- coding: utf-8 -*-
"""Transaction Classification FFNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_qF9iWd9VGtj_7_zA0PrqsvEVc6yLrQ

# DolFin Transaction Classification FFNN

This notebook is for training and testing a DolFin NLP model that will classify the transactions based on their description, the model has been developed with the intention of replacing the existing Basiq API which has provided poor classification results.We have implemented a Feed Forward Neural Network in this notebook.The following are the contents of the notbook:

* Feature Engineering

* Data preprocessing.

* Split the data into train, test, and validation.

* Developed a feed forward neural network to look at the developed numerical features and used it to classify the transactions.

* Incorporated TF callbacks to reduce overfitting.

* Compare the test performance of the NN to the NLP model.

According to the comparison and results, the LSTM model performed better than FFNN. The LSTM gave an accuracy of 98% on Test set and FFNN model gave a 70% accuracy. The FFNN model can be further worked on for improvng the accuracy.

Code by Samruddhi Bhor
"""

pip install tensorflow pandas nltk wordcloud

# Processing Utilities
import datetime
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Natural Language Processing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Deep learning
import tensorflow as tf
from tensorflow import keras
from keras import Model, layers

# Plotting utillities
import matplotlib.pyplot as plt
import seaborn as sns

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Data location, batch and image size
dataset_dir = '/content/drive/MyDrive/Colab Notebooks/Projects/'
# file_name = 'transaction_ut.csv'
file_name = 'fake_bank_account_transactions.csv'

# Load the dataset into a dataframe
df = pd.read_csv(dataset_dir + file_name)

# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)

# Store the column names
labels = list(df.columns)

# Determine the shape of the data
num_samples = df.shape[0]
num_features = df.shape[1]

msg = f'The file {file_name} contains {num_features} features and {num_samples} samples \n'
msg += f'The column names are: {labels}'
print(msg)

"""## Data Pre-processing"""

# Rename the column from "Category 3" to "class"
df.rename(columns={'Category 3': 'class'}, inplace=True)

#!pip install category_encoders

# # Generate additional features
df['Description_Length'] = df['Description'].apply(lambda x: len(x))

df['Transaction_Type'] = df.apply(lambda x: 'Debit' if x['DR/CR'] == 'DR' else 'Credit', axis=1)

# df['Transaction_Amount'] = df.apply(lambda x: x['Debit'] if x['DR/CR'] == 'DR' else x['Credit'], axis=1)

df.info()

import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Define stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# TF-IDF vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=1000)
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Description'])

# Add TF-IDF features to DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# sentiment analysis (using TextBlob library)
from textblob import TextBlob
df['Sentiment_Score'] = df['Description'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Example of topic modeling (using Latent Dirichlet Allocation)
from sklearn.decomposition import LatentDirichletAllocation
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_features = lda_model.fit_transform(tfidf_matrix)
df['Topic'] = lda_features.argmax(axis=1)

# Convert 'Date' column to datetime with the correct format
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')

# # Extract day of the week
df['Day_of_Week'] = df['Date'].dt.dayofweek

df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder

# Function to preprocess data
def preprocess_data(df):
    # Drop the 'Description' feature
    df_processed = df.drop(columns=['Description', 'Date','Consolidated','DR/CR','Transaction_Type' ])

    # Encode 'Transaction_Type' as binary values
    #df_processed['Transaction_Type'] = df_processed['Transaction_Type'].map({'Debit': 0, 'Credit': 1})
    #df_processed['DR/CR'] = df_processed['DR/CR'].map({'CR': 0, 'CR': 1})

    # One-hot encode 'Category 1' and 'Category 2' columns
    df_processed = pd.get_dummies(df_processed, columns=['Catgory 1', 'Category 2'], drop_first=True, dtype='int')

    # Scale numerical features using Min-Max scaling
    scaler = MinMaxScaler()
    numerical_features = ['Debit', 'Credit', 'Balance']
    #, 'Description_Length', 'Transaction_Amount']
    df_processed[numerical_features] = scaler.fit_transform(df_processed[numerical_features])

    return df_processed

# Preprocess the data
df_preprocessed = preprocess_data(df)

# Split the data into features (X) and target (y)
X = df_preprocessed.drop(columns=['class'])
y = df_preprocessed['class']

# List of columns to convert
columns_to_convert = ['Debit', 'Credit', 'Balance','Sentiment_Score']
#, 'Description_Length', , 'Transaction_Amount']

# Convert each column to int
for column in columns_to_convert:
    X[column] = X[column].fillna(0).round().astype(int)

# Label encode the target variable
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Split the data into train, validation, and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=40)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=40)

X.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = X.corr()

# Create a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

X.info()

# Define the FFNN model

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping

from keras.layers import Dropout


ffnn_model = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.1),
    Dense(len(np.unique(y)), activation='softmax')
])


# Compile the model

optimizer = keras.optimizers.RMSprop(learning_rate=0.001)
ffnn_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#ffnn_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = ffnn_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Evaluate the model on test data
test_loss, test_accuracy = ffnn_model.evaluate(X_test, y_test)
print(f"FFNN Model - Test Accuracy: {test_accuracy}")